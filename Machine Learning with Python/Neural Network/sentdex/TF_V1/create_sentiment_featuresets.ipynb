{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://pythonprogramming.net/static/downloads/machine-learning-data/pos.txt\n",
        "!wget https://pythonprogramming.net/static/downloads/machine-learning-data/neg.txt"
      ],
      "metadata": {
        "id": "pJwu_bEvj_Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "8k7Z2NjikHYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g4sXE1VckAsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pickle\n",
        "from collections import Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "hm_lines = 100000\n",
        "\n",
        "def create_lexicon(pos, neg):\n",
        "    \"\"\"\n",
        "    Create a lexicon from positive and negative files.\n",
        "    \"\"\"\n",
        "    \"\"\"\" Return word and their count \"\"\"\n",
        "    lexicon = []\n",
        "    for file in [pos, neg]:\n",
        "        with open(file, 'r') as f:\n",
        "            contents = f.readlines()\n",
        "            for l in contents[:hm_lines]:\n",
        "                all_words = word_tokenize(l)\n",
        "                lexicon.extend(all_words)\n",
        "    lexicon = [lemmatizer.lemmatize(i.lower()) for i in lexicon]\n",
        "    w_counts = Counter(lexicon)\n",
        "    lexicon = [w for w, c in w_counts.items() if 50 < c < 1000]\n",
        "    print(len(lexicon))\n",
        "    return lexicon\n",
        "\n",
        "def sample_handling(sample, lexicon, classification):\n",
        "    \"\"\" This function will contvert words in to vectors\n",
        "        featureset = [\n",
        "            [ [1 2 4 1], [0,1] ], <- negative sample\n",
        "            [ [0 1 0 1 1 0], [1,0] ] <- positive sample\n",
        "        ]\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    Handle sample data and create features.\n",
        "    \"\"\"\n",
        "    featureset = []\n",
        "    with open(sample, 'r') as f:\n",
        "        contents = f.readlines()\n",
        "        for l in contents[:hm_lines]:\n",
        "            current_words = word_tokenize(l.lower())\n",
        "            current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
        "            features = np.zeros(len(lexicon), dtype=int)\n",
        "            for word in current_words:\n",
        "                if word in lexicon:\n",
        "                    index_value = lexicon.index(word)\n",
        "                    features[index_value] += 1\n",
        "            featureset.append([features, classification])\n",
        "    return featureset\n",
        "\n",
        "def create_feature_sets_and_labels(pos, neg, test_size=0.1):\n",
        "    \"\"\"\n",
        "    Create feature sets and labels.\n",
        "    \"\"\"\n",
        "    lexicon = create_lexicon(pos, neg)\n",
        "    features = sample_handling(pos, lexicon, [1, 0]) + sample_handling(neg, lexicon, [0, 1])\n",
        "    random.shuffle(features)\n",
        "    features = np.array(features, dtype=object)\n",
        "\n",
        "    testing_size = int(test_size * len(features))\n",
        "\n",
        "    train_x, train_y = features[:-testing_size, 0], features[:-testing_size, 1]\n",
        "    test_x, test_y = features[-testing_size:, 0], features[-testing_size:, 1]\n",
        "\n",
        "    return train_x, train_y, test_x, test_y"
      ],
      "metadata": {
        "id": "8o4o6ajlmxc2"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x,train_y,test_x,test_y = create_feature_sets_and_labels('pos.txt','neg.txt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id5MfcfZotA-",
        "outputId": "7ae92ae7-fe01-4a83-cde9-efb17c98fdfd"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y1Kjbigro1V2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}